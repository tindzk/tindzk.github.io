<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
        <id>http://nieradzik.me/</id>
        <title>tindzk's blog</title>
        <subtitle>Blog about Scala and NLP</subtitle>
        <updated>2018-08-07T00:00:00+02:00</updated>
        <author>
          <name>Tim Nieradzik</name>
        </author>
        <link href="http://nieradzik.me/articles.xml" rel="self"></link>
        <link href="http://nieradzik.me/"></link>
        <entry>
          <id>http://nieradzik.me/transliteration.html</id>
          <title xml:lang="en-GB">Transliteration techniques</title>
          <published>2018-09-04T00:00:00+02:00</published>
          <updated>2018-09-04T00:00:00+02:00</updated>
          <link href="http://nieradzik.me/transliteration.html"></link>
          <category term="nlp"></category>
          <content type="xhtml" xml:lang="en-GB" xml:base="http://nieradzik.me/">
            <div xmlns="http://www.w3.org/1999/xhtml">
              <h2 id="introduction">Introduction</h2><p>The word <i>transliteration</i> is derived from the morphemes <i>trans-</i> (across, through) and <i>liter-</i> (letter). It denotes the translation from one alphabetic writing system into another. As an example, the city of Харків in Ukraine written in the Cyrillic script is commonly transliterated as either <i>Kharkiv</i> or <i>Harkiv</i> in the Latin script. This single example shows that there is no single mapping. The standard alphabet of any language is always optimised to accommodate its syntactic peculiarities.</p><p>In this article, we will look at use cases for transliteration and explore various considerations to be made when designing rules. To this end, I will present the <a href="https://github.com/sparsetech/translit-scala">translit-scala</a> library. Its development informed many of the examples and best practices outlined in this article.</p><h2 id="use-cases">Use cases</h2><p>The most common use of transliteration is for foreign entities. In Russian, the name <i>Steve Jobs</i> would be written as <i>Стив Джобс</i>. It does not adhere to the English spelling, but rather to the sounds. This phenomenon also applies to cities, products and other non-Russian names. The motivation is that it allows for a more natural integration of words and their adaptation to Russian grammar, e.g. declension. For example, <i>Steve Jobs' book</i> is translated as <i>книга Стива Джобса</i>. The word suffix <span class="code">-а</span> is a case marker which has the same possessive role as the <b>'s</b> or <b>s'</b> in English. In Latin-script Slavic languages the original spelling of foreign words is retained and the case marker is added. In Polish, the convention is to use an apostrophe to modify words with a trailing vowel sound: <i>Steve'a Jobsa</i> or <i>Harry'ego Pottera</i>.</p><p>International passports are the most prominent use of transliteration in the other direction. These contain a person's name in the original (e.g. Cyrillic) and Latin script. To avoid ambiguities, governments issue rules on how to correctly transliterate names.</p><p>Another use case are domain names. Although Unicode domains are widely supported, Latin-script domains are significantly more widespread. The same goes for URLs. Unicode characters can be used in URLs too, but in both cases, typing in a Unicode domain or URL would require changing the keyboard layout. An even more fundamental assumption is that the user is proficient in the alphabet, which is not always the case.</p><p>Search Engine Optimisation (SEO) is about improving the visibility of a website in search engines. Users who search for content using transliterated letters will receive more results if website content is transliterated. Furthermore, when copying links, browsers encode them as follows: <span class="code">https://ru.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B8%D1%81%D0%BE%D0%BA_%D0%BB%D0%B0%D1%82%D0%B8%D0%BD%D1%81%D0%BA%D0%B8%D1%85_%D0%B1%D1%83%D0%BA%D0%B2</span> With transliteration, the URL would still be readable and considerably shorter, e.g. <span class="code">https://ru.wikipedia.org/wiki/Spisok_latinskih_bukv</span>. Server-side software provides the ability to set up an alias, so both links would be valid.</p><p>In Natural Language Processing (NLP), there are two use cases: Machine Translation and disambiguation. Foreign names are often transliterated, as seen before. Disambiguation is the reverse operation: for example, in a travel or e-commerce chatbot, users may write a product or city name in either the original or transliterated spelling. Rather than accepting only exact matches, the chatbot could generate candidate transliterations.</p><p>Two characters looking exactly the same may still denote different letters. For example, p denotes "Latin Small Letter P" and has the code U+0070, whereas р is "Cyrillic Small Letter Er" and its code is U+0440. Confusing two similar-looking letters from different scripts is likely to happen when dealing with multiple keyboards and languages. This underlines the importance of disambiguation of user input, such as in the chatbot example.</p><p>Finally, transliteration could prove useful in assistive technology. When starting to learn a language, seeing transliterated words will help users acquire the new alphabet faster. Having a special transliteration keyboard could be advantageous to language learners, as mastering a new keyboard takes time. This also avoids mental switching, and could be even useful for non-English native speakers who are mostly working in an English context.</p><p>All these use cases have in common that they increase accessibility in one way or another, which is why transliteration can be seen as a valuable augmentation of a language's original script.</p><h2 id="classification">Classification</h2><p>Transliteration can be reduced to a translation problem. It concerns itself with the question of how to map letters from one alphabet into another. Transliteration to Latin is also called <i>romanisation</i>, derived from <i>Romance</i> languages, e.g. Italian, French, Spanish, etc.</p><p>Transliteration is a challenging problem as there is not always a direct equivalent for letters from two distinct alphabets. As an example, English has 26 letters, whereas Russian uses the Cyrillic alphabet and has 33. Other languages like Ukrainian, Bulgarian and Serbian also use Cyrillic letters, but have subtle differences which even extend to the pronunciation of particular letters. For instance, the letter и is pronounced differently in Russian and Ukrainian.</p><p>We can conclude that unique rules must be specified for individual languages and they need to account not only for spelling, but also pronunciation differences.</p><h2 id="rule-based-approach">Rule-based approach</h2><p>Languages follow inherent patterns, such as certain letter and sound combinations being more common than others. The most accurate way to transcribe words in a language is to use its official alphabet, but by modeling syntactic and phonetic patterns we can project words even onto other language alphabets. Suffice it to say, there is no single ground of truth in transliteration. Models can be of arbitrary complexity, ranging from hand-engineered character-level replacement rules, to Machine Learning models.</p><p>Character-level replacement rules may look as follows:</p><ul><li>X → Х</li><li>а → a</li><li>p → р</li><li>k → к</li><li>etc.</li></ul><p>This generally works well, but there are too many exceptions when considering single characters. Throughout this article, we will use a simple yet effective model that performs replacements of n-grams. For the languages at hand, bi- and trigrams cover a large proportion of vocabulary. These n-grams serve the purpose of approximating morphemes. As with other NLP tasks, these simple models tend to achieve accuracies competitive with more sophisticated Machine Learning models.</p><p>The upside of such a simple model based on n-gram pattern replacements is that it is straightforward to implement and reason about. With the advent of Machine Learning, researchers have also explored approaches based on Recurrent Neural Networks, which can even deal with noise in the input, such as spelling mistakes. These certainly have upsides, but equally require more computational resources and behave non-deterministically in some situations.</p><h3 id="rule-types">Rule types</h3><p>Rules can be derived by establishing a relationship between the source and target alphabet. A rule can be based on either:</p><ol><li><a href="https://en.wikipedia.org/wiki/Grapheme">Grapheme</a> similarity, or</li><li><a href="https://en.wikipedia.org/wiki/Phoneme">Phoneme</a> similarity</li></ol><p>An example for grapheme similarity is the Latin letter <i>m</i>, which could be mapped onto the Slavic <i>м</i>, or the letter <i>b</i>, which could be mapped onto <i>в</i>.</p><p>Phoneme similarity, on the other hand, establishes a relationship based on sounds: for example, the Cyrillic <i>x</i> could be mapped onto <i>h</i> (as in house) or <i>kh</i> (as in khaki). It is said that these two letters are homophones.</p><p>For instance, the Russian <i>hacker</i> (хакер) can be transliterated as either <i>xakep</i> (graphemes) or <i>haker</i> (phonemes).</p><p>The most intuitive approach is to default to phoneme similarity and provide additional convenience mappings based on grapheme similarity, for letters that would be unmapped otherwise.</p><h3 id="function-classes-and-directionality">Function classes and directionality</h3><p>In mathematics, the concept of function classes is established. A function that describes a mapping from one set to another can be either injective, surjective or bijective.</p><p>Bijective and surjective mappings make most sense in the context of transliteration, since every letter from the target alphabet (co-domain) should be mapped to by at least one letter from the source alphabet (domain).</p><p>Another concern is the directionality. If <span class="code">f(s)</span> maps a word from one alphabet <span class="code">S</span> onto another <span class="code">T</span> and <span class="code">g(t)</span> denotes the opposite direction with <span class="code">t ∈ T</span>, then the transliteration is said to be reversible if <span class="code">∀s ∈ S: s = g(f(s))</span>. This is only possible for bijective functions.</p><p>However, there are good reasons for surjective mappings, i.e. having multiple transliterations for a single letter. We can relax the constraint by normalising the input: <span class="code">∀s ∈ S: norm(s) = g(f(s))</span>. Thus, <span class="code">norm(s)</span> would define a primary transliteration for each character in <span class="code">s</span> and alternative (alias) ones. <span class="code">g(t)</span>, respectively, would have to return the primary transliteration for each character.</p><p>Note that the set of characters must comprise not only letters but also special diacritics such as accents and soft/hard signs. This poses some additional difficulties, but is worth accounting for.</p><p>The question of directionality is important, since government-issued rules often tend to be lossy and are only unidirectionally defined. This is especially true when choosing a phonetic approach, e.g. the <a href="http://zakon1.rada.gov.ua/laws/show/55-2010-%D0%BF">Ukrainian rule set</a> transliterates the name <i>Соломія</i> as <i>Solomiia</i>, with <i>і</i> mapped onto <i>і</i>, and <i>я</i> onto <i>ia</i>. However, <i>Юрій</i> is transliterated as <i>Yurii</i>. As can be seen from these examples, the Latin <i>i</i> has three possible transliterations depending on the context. These rules are difficult to reverse. With a few minor changes, e.g. encoding <i>я</i> using a less-overloaded letter (<i>ya</i>) and for <i>й</i> using a separate letter <i>j</i>, the complexity of the model can be reduced. Then, the names would be transliterated as <i>Yurij</i> and <i>Solomiya</i>, which are still sufficiently readable.</p><h3 id="conditional-probabilities">Conditional probabilities</h3><p>The morphology of each language obeys inherent patterns. Using a corpus, we can derive probabilities for how likely the next letter is given the context; formally, <span class="code">P(s | c)</span> where <span class="code">s</span> is any letter from the alphabet and <span class="code">c</span> the left context prior to <span class="code">s</span>.</p><p>If the target alphabet has more letters than the source alphabet, it may be desirable to re-use certain characters in different contexts and have the transliteration mechanism choose the correct one. As an alternative, one may use bi- or trigrams. A conditional model can still be useful to test hypotheses about the likelihood of certain n-grams occurring. In the event of the n-gram replacement being ambiguous, the model could correct mistakes without explicit feedback from the user, for example in the form of a precedence delimiter.</p><h3 id="considerations-and-trade-offs">Considerations and trade-offs</h3><p>When coming up with a set of rules, several factors are to be considered:</p><ul><li><b>Consistency:</b> There likely are already some established transliteration rules for a language. These may be taken as a basis.</li><li><b>Frequency:</b> The more common a letter is, the shorter its mapped n-gram should be.</li><li><b>Special characters:</b> Every character should be mapped, including soft/hard signs, apostrophes, etc. Otherwise, the transliteration is not reversible.</li><li><b>Conflicts:</b> Two Latin letters may correspond to several letters in the target language (homophones).</li><li><b>Keyboard position:</b> A common letter should be easy to type. The key travel distance of common sequences should be considered.</li><li><b>Shortcuts:</b> If there any unused Latin letters, these could be bound to long n-grams for convenience. For example, <i>q</i> could be mapped onto <i>щ</i> (shch) and <i>w</i> onto <i>ш</i>, as these two letters would have no meaning otherwise in Cyrillic languages.</li><li><b>Ambiguous mappings:</b> Can certain letters have several meanings in different contexts? Use an adaptive approach that looks at the context, or introduce a precedence separator for disambiguation.</li><li><b>Lookahead:</b> Does the rule require a lookahead, i.e. the right context? This should be avoided if possible as it makes on-the-fly transliteration harder.</li></ul><p>It turns out that we are looking at an optimisation problem that has many constraints. In my work so far, this optimisation step was performed manually by trial-and-error and using the transliteration schema in real-world scenarios. It would be valuable to explore automated approaches to find alternative, more optimal rules.</p><h2 id="implementation">Implementation</h2><p>The transliteration mechanism was implemented as a Scala library, which can be found <a href="http://github.com/sparsetech/translit-scala">here</a>. Currently, it has support for Russian and Ukrainian.</p><p>To limit the scope of this article, let us briefly explore how the Ukrainian alphabet is mapped. It is phonetic at its base, but with several extensions.</p><p>Unigrams:</p><table><thead><tr><th>a</th><th>b</th><th>d</th><th>e</th><th>f</th><th>g</th><th>h</th><th>i</th><th>j</th><th>k</th><th>l</th><th>m</th><th>n</th><th>o</th><th>p</th><th>r</th></tr></thead><tbody><tr><td>а</td><td>б</td><td>д</td><td>е</td><td>ф</td><td>г</td><td>х</td><td>і</td><td>й</td><td>к</td><td>л</td><td>м</td><td>н</td><td>о</td><td>п</td><td>р</td></tr></tbody></table><table><thead><tr><th>s</th><th>t</th><th>u</th><th>v</th><th>y</th><th>z</th></tr></thead><tbody><tr><td>с</td><td>т</td><td>у</td><td>в</td><td>и</td><td>з</td></tr></tbody></table><p>There are convenience mappings which are optimised for the English (US) keyboard layout:</p><table><thead><tr><th>c</th><th>q</th><th>w</th><th>x</th></tr></thead><tbody><tr><td>ц</td><td>щ</td><td>ш</td><td>ж</td></tr></tbody></table><p>Bigrams:</p><table><thead><tr><th>ya</th><th>ye</th><th>yi</th><th>yu</th><th>g'</th><th>ch</th><th>sh</th><th>ts</th><th>zh</th></tr></thead><tbody><tr><td>я</td><td>є</td><td>ї</td><td>ю</td><td>ґ</td><td>ч</td><td>ш</td><td>ц</td><td>ж</td></tr></tbody></table><p>There is a precedence separator, <span class="code">|</span>, which is used to prevent these bigrams from being applied. This is needed for some words such as <i>схильність</i> (<i>s|hyl'nist'</i>).</p><p>There are no trigrams, but there is one 4-gram:</p><table><thead><tr><th>shch</th></tr></thead><tbody><tr><td>щ</td></tr></tbody></table><p>Unlike Russian, Ukrainian does not have a hard sign. Instead, it has a soft sign and an apostrophe. There is experimental support for disambiguating apostrophes by looking at the context. This only works if the left and right context is available. For example, the word <i>п'ять</i> (English: <i>five</i>), corresponds to <i>p'yat'</i>. The first apostrophe stays an apostrophe, whereas the second becomes a soft sign.</p><p>The above rules converged after numerous modifications from the official <i>National 2010</i> government rules. For more information on the reasoning behind the mappings, please refer to the project page.</p><h3 id="library">Library</h3><p>The library can be used as follows:</p><pre class="sourceCode scala"><code data-lang="scala">translit.Ukrainian.latinToCyrillic("idu do domu")
  // іду до дому</code></pre><p>Under the hood, it uses the function <span class="code">latinToCyrillicOne</span> to transliterate the current character given its left and right context. For the second character from the input above, the call looks as follows:</p><pre class="sourceCode scala"><code data-lang="scala">translit.Ukrainian.latinToCyrillicOne("i", 'd', "u do domu")
  // (0, 'д')</code></pre><p>The first element of the tuple indicates how many characters relative to the offset need to be replaced by the mapped character. If it is a negative value, it indicates the number of characters in the left context.</p><p>For the input <i>shcho</i>/<i>що</i> (translation: 'what'), it would be desirable to receive on-the-fly replacements while the user is typing. Hence, the function would return the following values for each character:</p><pre class="sourceCode "><code data-lang="">( 0, 'с')
(-1, 'ш')
( 0, 'ц')
(-2, 'щ')
( 0, 'о')</code></pre><p>If these incremental changes are correctly applied, the output will be <i>що</i>.</p><p>So far, only the direction Latin to Cyrillic has been implemented.</p><p>The library is cross-compiled to JavaScript, JVM and LLVM bytecode, which allows the same logic to be used in a wide variety of languages.</p><h2 id="future-work">Future work</h2><h3 id="testing">Testing</h3><p>There are a number of unit tests that already ensure that the transliteration rules behave as expected. However, a corpus-linguistic approach would give further confidence.</p><p>After the reverse direction has been implemented, i.e. Cyrillic to Latin, we could use property-based testing and check whether the rules are well-behaved for a large corpus such as a Wikipedia text dump.</p><h3 id="optimisation">Optimisation</h3><p>There are <a href="http://mkweb.bcgsc.ca/carpalx/?typing_effort">endeavours</a> to optimise keyboard layouts, for example by modeling typing effort. While transliteration is agnostic to the keyboard layout, we can assume that it will be used with the QWERTY layout. The mappings could be optimised by taking into account texts from a large corpus, and measuring different metrics such as number of letters needed or finger travel distance for the letters in a word.</p><h3 id="machine-learning">Machine Learning</h3><p>So far the mapping rules have been manually defined. Another approach would be to use a sequence-to-sequence model such as LSTM. There are two use cases:</p><ol><li><b>Error correction:</b> This would allow users to make mistakes and diverge from the rules.</li><li><b>Disambiguation:</b> We could define more compact rules that use more unigrams rather than <span class="code">(n &gt; 1)</span>-grams. Handling all the exceptions and conflicts by hard-coding them is cumbersome.</li></ol><p>As demonstrated by Google Translate, these models can be error-prone and unpredictable, so they should be opt-in.</p><p>Some related work for using LSTM with Armenian can be found <a href="https://yerevann.github.io/2016/09/09/automatic-transliteration-with-lstm/">here</a>.</p><h3 id="contributing">Contributing</h3><p>The priority is to capture a large portion of use cases to make translit-scala production-ready. To help with this, feel free to point out any inconsistencies or examples where the current rules conflict.</p><p>So far, translit-scala only has a single set of rules for each language, but as end users have different mother tongues and use cases, it makes sense to support multiple layouts. Please feel free to contribute transliteration models for already-supported or other languages.</p>
            </div>
          </content>
        </entry><entry>
          <id>http://nieradzik.me/content-based-language-learning.html</id>
          <title xml:lang="en-GB">A content-based approach to language learning</title>
          <published>2018-08-07T00:00:00+02:00</published>
          <updated>2018-08-07T00:00:00+02:00</updated>
          <link href="http://nieradzik.me/content-based-language-learning.html"></link>
          <category term="nlp"></category>
          <content type="xhtml" xml:lang="en-GB" xml:base="http://nieradzik.me/">
            <div xmlns="http://www.w3.org/1999/xhtml">
              <h2 id="introduction">Introduction</h2><p>Language learning is rooted in experience. You start to excel in a language when you have been exposed to a wide variety of real-world situations. Therefore, full immersion is often advised, for example, by moving to a country where the language is spoken.</p><p>In this article, I will describe traditional approaches to language learning and conceptualise a content-based learning environment that makes use of assistive tooling. I will outline some of the challenges I encountered in implementing this approach and explore potential solutions.</p><h2 id="status-quo">Status quo</h2><p>Current approaches are centred around classroom teaching as well as self-learning courses or software. Both have their flaws. Classroom teaching offers an unnatural environment with little personalisation. There is a strong focus on grammatical rules and specific vocabulary. Furthermore, the content in textbooks that teachers use tends to be general and is rarely interesting.</p><p>Self-learning courses tend to teach the basics of a language but are hardly enough to satisfy the need to be able to speak a language fluently. Although there are more advanced courses, these only prove useful in sharpening one's understanding of the language and its subtleties.</p><p>Finally, there are numerous computer applications. These mostly target beginners and are thus slow-paced. Recent smartphone apps such as Duolingo aim to incorporate the concept of gamification, but suffer from the same insufficiency as classroom teaching and self-learning courses: lack of authentic content.</p><h2 id="a-content-based-approach">A content-based approach</h2><p>The initial interest in a language is piqued by the desire to use the language in real-world situations or consume specific literature, music or movies. As we saw in the previous section, this stands in stark contrast to the way most courses and tools are structured.</p><p>To stay motivated, learners could consume content aligned with their interests. Ideally, this would be content that they may also readily consume in their mother tongue if it was available in it.</p><p>Access to knowledge is the primary reason why people learn English and also succeed in it. Once the content moves to the foreground, as opposed to 'learning language X', eventual proficiency merely becomes a side-effect of continuously using the language.</p><p>This content-based approach could be classified as more applied than classroom teaching. Although studying vocabulary or grammar is not the focus of this methodology, these may form part of the process, but must be derived from the content the learner encounters.</p><h2 id="content-types">Content types</h2><p>We will look at the following four content types that are highly likely to be used in language learning:</p><ul><li>Movies</li><li>Books</li><li>Speech</li><li>Chat</li></ul><p>These types complement one another, creating an immersive learning environment that is made up of authentic content.</p><h3 id="movies">Movies</h3><p>Movies aim to capture the viewer's attention, which provides an incentive to understand the plot and persist in watching until the end. A common practice is to watch movies with English audio and the subtitles set in the target language. English words can be roughly correlated to the words in the subtitles. In the process, the viewer starts to recognise words that repeat themselves.</p><p>To gain more certainty about the meaning of words in the subtitles, it is still necessary to look them up and verify one's hypotheses. This involves switching the application and typing the word manually into a dictionary or Google Translate.</p><p>Seeing the written words and looking them up alone does not facilitate remembering. It is advisable to copy the word in its context to a text file for later reviewing.</p><p>Even though this method is effective and simple, the context switches of using a translation website and then copying over the sentence takes a considerable amount of time and is discouraging, especially for lengthy movies.</p><h3 id="books">Books</h3><p>On several occasions, I bought paperback books in a foreign language, most of which I gave up on after only 50-100 pages. The most common reason was the slow process of looking up the words. I would read the book and look up unfamiliar words on my smartphone. This process turned out to be rather tedious. As with movies, the context switches were a major distraction, although here it would require switching between completely different media: laying down the book and picking up the smartphone, and vice-versa.</p><p>The situation has been slightly improved by e-book readers. These are often equipped with a dictionary app. But these apps malfunction for any language other than English. When tapping a word, the base form would not be inferred.</p><p>Another downside is that keeping track of the words encountered is not as convenient as on the laptop and I would skip this step altogether, missing out on the ability to later review the new words.</p><h3 id="speech--podcasts-and-audiobooks">Speech: podcasts and audiobooks</h3><p>Self-teaching audio courses often gravitate towards productive rather than receptive language. I found these scripted dialogues and texts rather limiting. Natural speech is significantly more complex and knowing basic phrases is insufficient for having a regular conversation.</p><p>Therefore, I propose to consume solely audio content that is geared towards native speakers. In the process, better foundations are fostered than audio courses could possibly convey. The primary advantage is that this content type can be consumed in various contexts, for example on commutes or travels.</p><p>There are two promising candidates for natural speech: podcasts and audiobooks. Podcasts convey information that is of local interest, such as politics, culture, events, etc. These are often in the form of discussions and tend to be short, mostly between 20 and 45 minutes in duration. All these properties make them an excellent fit for language learning. One can immerse oneself in cultural and local topics. Also, the dialogue form of questions and answers allows one to form language hypotheses when the host asks a question and to test them when the guest answers it.</p><p>Audiobooks are significantly harder to understand as they use more literary language and dialogues are less frequent, though a combination of both content types might give the best results.</p><p>The problem I had with speech was that, unlike movies, there is no transcript (like subtitles) that can be followed, nor are there any smartphone apps that can show one in real-time.</p><h3 id="chat">Chat</h3><p>Becoming proficient in a new keyboard takes time and comes with the burden that the layout needs to be changed on the system level, which also impacts all other key combinations of software such as Vi. Also, chatting with people in different languages requires changing not only the system's keyboard layout, but also the browser's spell checker.</p><p>Many Slavic languages use the Cyrillic alphabet, which poses a major difficulty to learners. I would use Google Translate's transliteration capability to convert Latin letters to their Cyrillic equivalents. Unfortunately, it does not follow strict rules, so the transliterations are rather indeterministic.</p><p>Alternatively, one could use Latin letters when conversing with native speakers, but this will cause some confusion on the part of the interlocutor.</p><p>While Android phones have the ability to quickly switch the layout and spell checker, this feature is missing in browsers. Another useful feature of Android is its ability to auto-complete text. This was originally designed to speed up the writing process on touch screens, but it could aid language learners with using the right grammatical forms.</p><h2 id="potential-solutions">Potential solutions</h2><p>Most software can be extended using plug-ins, and open-source software by modifying their code. Here are some thoughts on how the workflow can be improved to accommodate the needs of language learners:</p><p>For movies, a plug-in for <a href="https://mpv.io/">mpv</a> could make subtitles clickable. It would look up the phrase or a specific word in Google Translate and remember it along with the start and end time of the utterance. Then, a background daemon could extract the audio segments.</p><p><a href="http://antennapod.org/">AntennaPod</a> for podcasts and <a href="https://github.com/PaulWoitaschek/Voice">Voice</a> for audiobooks are the Android apps of my choice. Both are open-source and could be extended with the ability to follow the transcription on the fly. The transcription with timings can be obtained by using the <a href="https://cloud.google.com/speech-to-text/">Cloud Speech-to-Text API</a>. This feature would allow the user to actively follow the spoken utterances and tap unknown words to look them up. As with movies, it could remember the timing and extract the context as an audio file.</p><p>For Android-based e-book readers, it is possible to modify <a href="https://github.com/geometer/FBReaderJ">FBReader</a> and equip it with an offline dictionary that can perform stemming. When tapping a word, it could look up the base form or use Google Translate to show context-specific translations.</p><p>For chat, a browser extension could integrate with Telegram and other messengers. Depending on the current interlocutor, it would choose the correct keyboard layout and perform transliteration if needed. It might even point out grammatical or stylistic errors.</p><p>Taking it a step further, a client-server architecture could be devised. Such an architecture should be agnostic to the content types. At the base, it could consist of a centralised server with an API that the different software integrations and plug-ins interact with. This would allow all the different data streams to be aggregated in a central place.</p><p>The server would store all audio samples, words, phrases and text snippets that the user encounters in different contexts, across different software and devices.</p><p>With this wealth of information, the server can generate personalised flashcards. This is presumably more effective than taking off-the-shelf decks as the learner has an active memory of the context in which the sentences appeared. Another advantage of combining different data sources is that the system could build a profile of the user and track their progress.</p><h2 id="conclusion">Conclusion</h2><p>I described how improvements in methodology and tooling could make language learning more effective.</p><p>At the moment, there is a general lack of integration between tools. As a consequence, there are context switches and repetitive, manual labour. The lack of assistive features such as auto-completions and the ability to look up words, remember audio segments or the textual context slow down learning with real content immensely.</p><p>So far I have only outlined one component of language learning, albeit the most important as it lays the foundations for one's understanding and ability to read, write and speak. Nevertheless, a content-based approach is incomplete without the study of rules and conversational practice.</p><p>I have experimented with some ideas and plan to develop these further. There are more content types to be explored and I hope to turn some of these into projects.</p>
            </div>
          </content>
        </entry>
      </feed>